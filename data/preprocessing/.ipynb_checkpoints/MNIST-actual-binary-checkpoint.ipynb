{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"C:/Users/sumaiyah/OneDrive - University Of Cambridge/Project/DNN-RE/src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'MNIST-actual-binary'\n",
    "target_col_name = 'digit'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Dataset\n",
    "MNIST handwritten digit letter recognition dataset  \n",
    "\n",
    "**Multi-classification:**\n",
    "- **Input:** Letters are represented with 8x8 image, 784 pixels \n",
    "- **Output:** 0-9, The digit\n",
    "\n",
    "**Binary classification:**\n",
    "- **Input:** Letters are represented with 8x8 image, 784 pixels \n",
    "- **Output:** 0 if digit=1 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data from tensorflow and preprocess\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Get all x and y data together\n",
    "# X = np.concatenate((x_train, x_test), axis=0).astype('float32')\n",
    "# y = np.concatenate((y_train, y_test), axis=0)\n",
    "X = np.array(x_train, dtype='float32')\n",
    "y = y_train\n",
    "\n",
    "# Normalizing the RGB codes by dividing it to the max RGB value\n",
    "X /= 255\n",
    "\n",
    "# Flatten the images\n",
    "image_vector_size = 28 * 28\n",
    "X = X.reshape(X.shape[0], image_vector_size)\n",
    "\n",
    "# Put data processed into dataframe\n",
    "data = pd.DataFrame(data=X, columns=[i + 1 for i in range(0, 784)])\n",
    "data['digit'] = y\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_digit = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of input features\n",
    "feature_col_names = list(data.columns)\n",
    "feature_col_names.remove(target_col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['digit'].isin([0,1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode target data\n",
    "# class_0 = target digit\n",
    "# class_1 = all other digits\n",
    "\n",
    "# Convert into binary classification task, convert numbers other than target into 1\n",
    "# Do not do this in place\n",
    "# class_0 = data[data[target_col_name] == target_digit].copy()\n",
    "# class_1 = data[data[target_col_name] != target_digit].copy().sample(frac=1).reset_index(drop=True)[0:len(class_0)]\n",
    "# class_0.loc[:, target_col_name] = 0\n",
    "# class_1.loc[:, target_col_name] = 1\n",
    "\n",
    "# # Shuffle all the data and combine into 1 dataframe\n",
    "# data = pd.concat((class_0, class_1)).sample(frac=1).reset_index(drop=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate input features and target column\n",
    "X = data.drop(columns=[target_col_name]).values  \n",
    "y = data[target_col_name].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Scale input features i.e. scale attributes so that theyre 0-1 so that larger weights do not carry more signifcance in the network\n",
    "scaler = MinMaxScaler() \n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store preprocessed data\n",
    "data = pd.DataFrame(X, columns=feature_col_names)\n",
    "data[target_col_name] = y\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert data.columns[-1]==target_col_name, 'Target column must be last column in DataFrame'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise new empty dataset folder\n",
    "from model.generation.helpers import init_dataset_dir\n",
    "\n",
    "path_to_data_folder = '../'\n",
    "init_dataset_dir.run(dataset_name=dataset_name, path_to_data_folder=path_to_data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../' + dataset_name + '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned data\n",
    "data.to_csv(data_path + 'data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
